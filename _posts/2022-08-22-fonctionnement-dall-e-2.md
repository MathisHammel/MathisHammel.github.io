---
layout: post
title:  "Le fonctionnement de DALLÂ·E 2 expliquÃ©"
date:   2022-08-22 10:18:59 +0200
image:  '/images/blog/fonctionnement-dall-e-2/header.png'
tags:   [IA]
---

Aujourd'hui, je vais vous fais dÃ©couvrir en 6 minutes le fonctionnement de DALLÂ·E 2, l'IA qui a rÃ©volutionnÃ© le machine learning en 2022. C'est parti ! â¤µï¸ 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561628989453733889-FawB8aRWYAEeJUj.jpg" draggable="false">
  </div>
</div>

Les rÃ©seaux de neurones peuvent paraÃ®tre super mystiques avec un nom qui fait peur, mais en fait il s'agit simplement d'un enchaÃ®nement de **plein d'opÃ©rations mathÃ©matiques simples** qui peuvent "apprendre" Ã  effectuer une tÃ¢che : par exemple, reconnaÃ®tre des objets dans une image. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561628993853571072-FawCTGRWYAAaI_z.jpg" draggable="false">
  </div>
</div>

L'entraÃ®nement typique d'un rÃ©seau de neurones se fait en lui montrant Ã©normÃ©ment (jusqu'Ã  **plusieurs milliards**) d'exemples en entrÃ©e et des rÃ©ponses attendues en sortie. Ã€ la maniÃ¨re du cerveau humain, il va modifier ses connexions internes pour apprendre ce qu'on attend de lui. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561628998752411648-FawCWYVWIAAjhd1.jpg" draggable="false">
  </div>
</div>

Avant de pouvoir Ã©tudier DALLÂ·E 2, il faut d'abord se pencher sur son "ancÃªtre", **CLIP**.

CLIP est un rÃ©seau de neurones inventÃ© en 2021 pour une tÃ¢che complexe : la reconnaissance d'images en mode "zero-shot". Le zero-shot, c'est la spÃ©cialitÃ© d'OpenAI.

D'habitude, pour apprendre une tÃ¢che spÃ©cifique, le rÃ©seau de neurones a besoin du dataset correspondant. Par exemple pour la reconnaissance de chiffres manuscrits, on peut utiliser le dataset **MNIST** qui contient 60.000 images d'entraÃ®nement : 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629005626966019-FawCgOGX0AIYgmG.jpg" draggable="false">
  </div>
</div>

La particularitÃ© de CLIP, c'est qu'il n'a **mÃªme pas besoin de ce genre d'entraÃ®nement** !

Il est en fait prÃ©-entraÃ®nÃ© sur un dataset gigantesque et trÃ¨s diversifiÃ©, donc il peut s'en sortir sur plein de tÃ¢ches de reconnaissance d'images **sans voir de donnÃ©es spÃ©cifiques** Ã  la tÃ¢che.

Les modÃ¨les zero-shot sont ultra puissants : par exemple, lÃ  oÃ¹ un modÃ¨le classique saura dÃ©tecter au maximum quelques milliers d'objets diffÃ©rents (sur lequel il aura subi un entraÃ®nement prÃ©alable), CLIP peut en reconnaÃ®tre **une infinitÃ©**. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629012266557440-FawCsXPXoAIZdWT.jpg" draggable="false">
  </div>
</div>

Pour entraÃ®ner CLIP, les chercheursÂ·es du labo OpenAI ont crawlÃ© internet pour tÃ©lÃ©charger **400 millions d'images** sous licence libre, sur Flickr notamment.

La description de chaque photo est aussi rÃ©cupÃ©rÃ©e, pour avoir un grand nombre de correspondances image/texte.

Ã€ partir de cet immense dataset, on entraÃ®ne deux rÃ©seaux de neurones appelÃ©s encodeurs, qui vont chacun calculer une sorte de hash. L'un des deux travaille sur les images, l'autre sur le texte.

Ce hash est appelÃ© "**embedding**", c'est un tableau de nombres. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629018692222976-FawC0yjWAAEJ0Yw.jpg" draggable="false">
  </div>
</div>

Les valeurs de ces nombres n'ont pas vraiment de sens pour les humains. Mais pour l'IA qui va les crÃ©er, leur composition traduit trÃ¨s bien le sens d'une image ou d'une phrase.

En quelque sorte, CLIP **invente sa propre langue** optimisÃ©e pour dÃ©crire des images !

Pour Ãªtre utiles, il faut donc que les deux rÃ©seaux de neurones respectent des contraintes sur les embeddings :

- Si l'image correspond au texte, leurs embeddings doivent Ãªtre **quasi-identiques**
- Si elle ne correspond pas, leurs embeddings doivent Ãªtre **trÃ¨s diffÃ©rents **

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629025856110593-FawC_umXgAAx2YP.jpg" draggable="false">
  </div>
</div>

L'entraÃ®nement est effectuÃ© simultanÃ©ment sur les deux rÃ©seaux pour que les deux contraintes ci-dessus soient respectÃ©es.

La structure du langage commun aux deux encodeurs, aussi appelÃ© "**espace latent**", va se former naturellement.

Je vous disais plus tÃ´t que l'entraÃ®nement se fait traditionnellement en montrant au rÃ©seau des exemples d'entrÃ©es+sorties, ici c'est donc un peu diffÃ©rent : on prÃ©sente deux entrÃ©es Ã  CLIP, et on lui dit "dÃ©brouille-toi pour que leurs embeddings correspondent".

Pour utiliser moins de puissance de calcul, on groupe les images par lots de 32768, puis on calcule leurs embeddings ainsi que ceux de leurs descriptions.

En les comparant deux Ã  deux, on obtient plus d'un milliard de paires d'entraÃ®nement image/texte (positives et nÃ©gatives). 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629034651549696-FawDON-WYAAPUNR.jpg" draggable="false">
  </div>
</div>

Bon. On a maintenant deux rÃ©seaux de neurones encodeurs, qui savent ensemble dire quelle image correspond Ã  quelle description. Mais Ã  quoi Ã§a sert en fait ?

Une fois entraÃ®nÃ©, la capacitÃ© de CLIP peut Ãªtre utilisÃ©e pour classifier des images jamais vues auparavant.

Pour cela, on commence par calculer **l'embedding de l'image** que l'on cherche Ã  classifier.

Ensuite, on calcule **tous les embeddings des textes correspondant aux classes possibles**, et on regarde lequel est le plus similaire Ã  celui de l'image. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629041358159876-FawDUbwXgAAD_zn.jpg" draggable="false">
  </div>
</div>

Comme CLIP aura de grandes chances d'avoir vu des exemples similaires pendant son entraÃ®nement, il devrait pouvoir reconnaÃ®tre l'objet parmi la liste des propositions alors mÃªme qu'on ne lui a jamais prÃ©sentÃ© d'exemples explicites au prÃ©alable !

Maintenant, c'est quoi le rapport entre CLIP et DALLÂ·E 2 ?

Vous allez voir que le pouvoir des encodeurs de CLIP ne s'arrÃªte pas lÃ , et qu'ils sont aussi au cÅ“ur du fonctionnement de DALLÂ·E 2 !

Ce qui fait toute la puissance de DALLÂ·E 2, c'est l'introduction d'un **troisiÃ¨me rÃ©seau de neurones** : un dÃ©codeur appelÃ© **unCLIP**, qui arrive Ã  **reconstituer une image Ã  partir de son embedding**. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629050636058627-FawDgeiXEAAlNlF.jpg" draggable="false">
  </div>
</div>

L'entraÃ®nement de celui-ci est plus complexe, c'est un **modÃ¨le de diffusion guidÃ©**. Je ne vais pas entrer dans les dÃ©tails aujourd'hui, mais ce sont des modÃ¨les trÃ¨s puissants pour gÃ©nÃ©rer des images rÃ©alistes. Si vous connaissez les GAN, c'est pareil mais en mieux ğŸ˜

Alors bien Ã©videmment, unCLIP ne reproduit pas un rÃ©sultat parfait (sinon, on a inventÃ© un algorithme de compression thÃ©oriquement impossible) : le rÃ©seau peut seulement produire une image plausible parmi tant d'autres. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629058412208131-FawDyTSWYAAzj4g.jpg" draggable="false">
  </div>
</div>

Mais comme la reprÃ©sentation des embeddings contient quand mÃªme de l'information sÃ©mantique condensÃ©e (dans la langue inventÃ©e par CLIP durant son entraÃ®nement), on peut reconstituer l'image Ã  partir de son embedding en gardant son aspect et sa structure. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629063927799808-FawEABsXEAA9erY.jpg" draggable="false">
  </div>
</div>

En voyant cette imperfection comme une maniÃ¨re de crÃ©er de la diversitÃ©, on peut d'ailleurs utiliser unCLIP pour gÃ©nÃ©rer des variations sur une image.

Et si on rÃ©pÃ¨te le processus en boucle, on peut crÃ©er de magnifiques animations abstraites comme l'a fait Alan Resnick : 

<div class="gallery-box">
  <div class="gallery">
<video controls>  <source src="/images/blog/fonctionnement-dall-e-2/1561629741651730432-Kn_22WEPat1RuiEg.mp4" type="video/mp4"></video>  </div>
</div>

On peut aussi s'en servir pour **mÃ©langer deux images** qui n'ont rien Ã  voir !

Est-ce que vous vous Ãªtes dÃ©jÃ  demandÃ©s Ã  quoi pouvait ressembler une image composÃ©e Ã  30% de corgis et Ã  70% d'une peinture de Van Gogh ? Probablement pas, mais voici la rÃ©ponse au cas oÃ¹ : 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629747725144065-FawEaXiX0AEsRCN.jpg" draggable="false">
  </div>
</div>

Mais ce qui a fait la cÃ©lÃ©britÃ© de cette IA, c'est bien Ã©videmment sa capacitÃ© Ã  gÃ©nÃ©rer des images correspondant Ã  n'importe quel texte.

Pour Ã§a, il suffit Ã  DALLÂ·E 2 d'encoder le texte fourni, puis de dÃ©coder l'embedding rÃ©sultant pour en faire une image rÃ©aliste. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629752443772930-FawE57KWAAEyaR1.jpg" draggable="false">
  </div>
</div>

Maintenant, Ã  vous de jouer !

DALLÂ·E 2 est encore en beta privÃ©e sur le site d'OpenAI, en attendant sa sortie publique vous pouvez vous amuser sur [http://crAIyon.com](http://crAIyon.com). Son modÃ¨le est moins puissant mais c'est dÃ©jÃ  impressionnant.

Envoyez-moi vos meilleures crÃ©ations ğŸ™‚ 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561630271241428994-FawGLOAXoAAPfQO.jpg" draggable="false">
  </div>
</div>

C'est la fin de cet article, bravo d'avoir tenu jusqu'au bout ! J'espÃ¨re que Ã§a vous a aidÃ© Ã  comprendre ces deux modÃ¨les Ã  la pointe de la recherche en machine learning.

Pour moi, c'Ã©tait l'un des exercices de vulgarisation les plus difficiles de ma vie ğŸ˜„

Avant de partir, n'hÃ©sitez pas Ã  partager et me suivre sur Twitter pour lire mes threads rÃ©guliers de vulgarisation tech (c'est moi qui ai piratÃ© l'appli Elyze, dÃ©bunkÃ© le quiz Ã©lectoral de Zemmour et obtenu le pass sanitaire de Jean Castex ğŸ˜‡).

Pour aller plus loin :

- Le papier de recherche de CLIP [https://arxiv.org/pdf/2103.00020...](https://arxiv.org/pdf/2103.00020.pdf) et DALLÂ·E 2 [https://cdn.openai.com/papers/da...](https://cdn.openai.com/papers/dall-e-2.pdf)
- Une playlist incroyable qui vulgarise les rÃ©seaux de neurones [https://www.youtube.com/watch?v=...](https://www.youtube.com/watch?v=aircAruvnKk)
- Mon [dernier article](/blog/dall-e-2-triche) sur les biais dans l'IA

