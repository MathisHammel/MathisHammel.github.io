---
layout: post
title:  "Le fonctionnement de DALL¬∑E 2 expliqu√©"
date:   2022-08-22 10:18:59 +0200
image:  '/images/blog/fonctionnement-dall-e-2/header.png'
tags:   [IA]
---

Aujourd'hui, je vais vous fais d√©couvrir en 6 minutes le fonctionnement de DALL¬∑E 2, l'IA qui a r√©volutionn√© le machine learning en 2022. C'est parti ! ‚§µÔ∏è 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561628989453733889-FawB8aRWYAEeJUj.jpg" draggable="false">
  </div>
</div>

Les r√©seaux de neurones peuvent para√Ætre super mystiques avec un nom qui fait peur, mais en fait il s'agit simplement d'un encha√Ænement de **plein d'op√©rations math√©matiques simples** qui peuvent "apprendre" √† effectuer une t√¢che : par exemple, reconna√Ætre des objets dans une image. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561628993853571072-FawCTGRWYAAaI_z.jpg" draggable="false">
  </div>
</div>

L'entra√Ænement typique d'un r√©seau de neurones se fait en lui montrant √©norm√©ment (jusqu'√† **plusieurs milliards**) d'exemples en entr√©e et des r√©ponses attendues en sortie. √Ä la mani√®re du cerveau humain, il va modifier ses connexions internes pour apprendre ce qu'on attend de lui. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561628998752411648-FawCWYVWIAAjhd1.jpg" draggable="false">
  </div>
</div>

Avant de pouvoir √©tudier DALL¬∑E 2, il faut d'abord se pencher sur son "anc√™tre", **CLIP**.

CLIP est un r√©seau de neurones invent√© en 2021 pour une t√¢che complexe : la reconnaissance d'images en mode "zero-shot". Le zero-shot, c'est la sp√©cialit√© d'OpenAI.

D'habitude, pour apprendre une t√¢che sp√©cifique, le r√©seau de neurones a besoin du dataset correspondant. Par exemple pour la reconnaissance de chiffres manuscrits, on peut utiliser le dataset **MNIST** qui contient 60.000 images d'entra√Ænement : 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629005626966019-FawCgOGX0AIYgmG.jpg" draggable="false">
  </div>
</div>

La particularit√© de CLIP, c'est qu'il n'a **m√™me pas besoin de ce genre d'entra√Ænement** !

Il est en fait pr√©-entra√Æn√© sur un dataset gigantesque et tr√®s diversifi√©, donc il peut s'en sortir sur plein de t√¢ches de reconnaissance d'images **sans voir de donn√©es sp√©cifiques** √† la t√¢che.

Les mod√®les zero-shot sont ultra puissants : par exemple, l√† o√π un mod√®le classique saura d√©tecter au maximum quelques milliers d'objets diff√©rents (sur lequel il aura subi un entra√Ænement pr√©alable), CLIP peut en reconna√Ætre **une infinit√©**. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629012266557440-FawCsXPXoAIZdWT.jpg" draggable="false">
  </div>
</div>

Pour entra√Æner CLIP, les chercheurs¬∑es du labo OpenAI ont crawl√© internet pour t√©l√©charger **400 millions d'images** sous licence libre, sur Flickr notamment.

La description de chaque photo est aussi r√©cup√©r√©e, pour avoir un grand nombre de correspondances image/texte.

√Ä partir de cet immense dataset, on entra√Æne deux r√©seaux de neurones appel√©s encodeurs, qui vont chacun calculer une sorte de hash. L'un des deux travaille sur les images, l'autre sur le texte.

Ce hash est appel√© "**embedding**", c'est un tableau de nombres. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629018692222976-FawC0yjWAAEJ0Yw.jpg" draggable="false">
  </div>
</div>

Les valeurs de ces nombres n'ont pas vraiment de sens pour les humains. Mais pour l'IA qui va les cr√©er, leur composition traduit tr√®s bien le sens d'une image ou d'une phrase.

En quelque sorte, CLIP **invente sa propre langue** optimis√©e pour d√©crire des images !

Pour √™tre utiles, il faut donc que les deux r√©seaux de neurones respectent des contraintes sur les embeddings :

- Si l'image correspond au texte, leurs embeddings doivent √™tre **quasi-identiques**
- Si elle ne correspond pas, leurs embeddings doivent √™tre **tr√®s diff√©rents**

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629025856110593-FawC_umXgAAx2YP.jpg" draggable="false">
  </div>
</div>

L'entra√Ænement est effectu√© simultan√©ment sur les deux r√©seaux pour que les deux contraintes ci-dessus soient respect√©es.

La structure du langage commun aux deux encodeurs, aussi appel√© "**espace latent**", va se former naturellement.

Je vous disais plus t√¥t que l'entra√Ænement se fait traditionnellement en montrant au r√©seau des exemples d'entr√©es+sorties, ici c'est donc un peu diff√©rent : on pr√©sente deux entr√©es √† CLIP, et on lui dit "d√©brouille-toi pour que leurs embeddings correspondent".

Pour utiliser moins de puissance de calcul, on groupe les images par lots de 32768, puis on calcule leurs embeddings ainsi que ceux de leurs descriptions.

En les comparant deux √† deux, on obtient plus d'un milliard de paires d'entra√Ænement image/texte (positives et n√©gatives). 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629034651549696-FawDON-WYAAPUNR.jpg" draggable="false">
  </div>
</div>

Bon. On a maintenant deux r√©seaux de neurones encodeurs, qui savent ensemble dire quelle image correspond √† quelle description. Mais √† quoi √ßa sert en fait ?

Une fois entra√Æn√©, la capacit√© de CLIP peut √™tre utilis√©e pour classifier des images jamais vues auparavant.

Pour cela, on commence par calculer **l'embedding de l'image** que l'on cherche √† classifier.

Ensuite, on calcule **tous les embeddings des textes correspondant aux classes possibles**, et on regarde lequel est le plus similaire √† celui de l'image. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629041358159876-FawDUbwXgAAD_zn.jpg" draggable="false">
  </div>
</div>

Comme CLIP aura de grandes chances d'avoir vu des exemples similaires pendant son entra√Ænement, il devrait pouvoir reconna√Ætre l'objet parmi la liste des propositions alors m√™me qu'on ne lui a jamais pr√©sent√© d'exemples explicites au pr√©alable !

Maintenant, c'est quoi le rapport entre CLIP et DALL¬∑E 2 ?

Vous allez voir que le pouvoir des encodeurs de CLIP ne s'arr√™te pas l√†, et qu'ils sont aussi au c≈ìur du fonctionnement de DALL¬∑E 2 !

Ce qui fait toute la puissance de DALL¬∑E 2, c'est l'introduction d'un **troisi√®me r√©seau de neurones** : un d√©codeur appel√© **unCLIP**, qui arrive √† **reconstituer une image √† partir de son embedding**. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629050636058627-FawDgeiXEAAlNlF.jpg" draggable="false">
  </div>
</div>

L'entra√Ænement de celui-ci est plus complexe, c'est un **mod√®le de diffusion guid√©**. Je ne vais pas entrer dans les d√©tails aujourd'hui, mais ce sont des mod√®les tr√®s puissants pour g√©n√©rer des images r√©alistes. Si vous connaissez les GAN, c'est pareil mais en mieux üòÅ

Alors bien √©videmment, unCLIP ne reproduit pas un r√©sultat parfait (sinon, on a invent√© un algorithme de compression th√©oriquement impossible) : le r√©seau peut seulement produire une image plausible parmi tant d'autres. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629058412208131-FawDyTSWYAAzj4g.jpg" draggable="false">
  </div>
</div>

Mais comme la repr√©sentation des embeddings contient quand m√™me de l'information s√©mantique condens√©e (dans la langue invent√©e par CLIP durant son entra√Ænement), on peut reconstituer l'image √† partir de son embedding en gardant son aspect et sa structure. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629063927799808-FawEABsXEAA9erY.jpg" draggable="false">
  </div>
</div>

En voyant cette imperfection comme une mani√®re de cr√©er de la diversit√©, on peut d'ailleurs utiliser unCLIP pour g√©n√©rer des variations sur une image.

Et si on r√©p√®te le processus en boucle, on peut cr√©er de magnifiques animations abstraites comme l'a fait Alan Resnick : 

<div class="gallery-box">
  <div class="gallery">
<video controls>  <source src="/images/blog/fonctionnement-dall-e-2/1561629741651730432-Kn_22WEPat1RuiEg.mp4" type="video/mp4"></video>  </div>
</div>

On peut aussi s'en servir pour **m√©langer deux images** qui n'ont rien √† voir !

Est-ce que vous vous √™tes d√©j√† demand√©s √† quoi pouvait ressembler une image compos√©e √† 30% de corgis et √† 70% d'une peinture de Van Gogh ? Probablement pas, mais voici la r√©ponse au cas o√π : 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629747725144065-FawEaXiX0AEsRCN.jpg" draggable="false">
  </div>
</div>

Mais ce qui a fait la c√©l√©brit√© de cette IA, c'est bien √©videmment sa capacit√© √† g√©n√©rer des images correspondant √† n'importe quel texte.

Pour √ßa, il suffit √† DALL¬∑E 2 d'encoder le texte fourni, puis de d√©coder l'embedding r√©sultant pour en faire une image r√©aliste. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561629752443772930-FawE57KWAAEyaR1.jpg" draggable="false">
  </div>
</div>

Maintenant, √† vous de jouer !

DALL¬∑E 2 est encore en beta priv√©e sur le site d'OpenAI, en attendant sa sortie publique vous pouvez vous amuser sur [http://crAIyon.com](http://crAIyon.com). Son mod√®le est moins puissant mais c'est d√©j√† impressionnant.

Envoyez-moi vos meilleures cr√©ations üôÇ 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/fonctionnement-dall-e-2/1561630271241428994-FawGLOAXoAAPfQO.jpg" draggable="false">
  </div>
</div>

C'est la fin de cet article, bravo d'avoir tenu jusqu'au bout ! J'esp√®re que √ßa vous a aid√© √† comprendre ces deux mod√®les √† la pointe de la recherche en machine learning.

Pour moi, c'√©tait l'un des exercices de vulgarisation les plus difficiles de ma vie üòÑ

Avant de partir, n'h√©sitez pas √† partager et me suivre sur Twitter pour lire mes threads r√©guliers de vulgarisation tech (c'est moi qui ai pirat√© l'appli Elyze, d√©bunk√© le quiz √©lectoral de Zemmour et obtenu le pass sanitaire de Jean Castex üòá).

Pour aller plus loin :

- Le papier de recherche de CLIP [https://arxiv.org/pdf/2103.00020...](https://arxiv.org/pdf/2103.00020.pdf) et DALL¬∑E 2 [https://cdn.openai.com/papers/da...](https://cdn.openai.com/papers/dall-e-2.pdf)
- Une playlist incroyable qui vulgarise les r√©seaux de neurones [https://www.youtube.com/watch?v=...](https://www.youtube.com/watch?v=aircAruvnKk)
- Mon [dernier article](/blog/dall-e-2-triche) sur les biais dans l'IA

